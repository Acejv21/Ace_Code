1.) Why is using one-hot encoding an inefficient towards vectorizing a corpus of words?  How are word embeddings different? (see this video https://www.youtube.com/watch?v=EEk6OiOOT2c )

A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine we have 10,000 words in the vocabulary. To one-hot encode each word, we would create a vector where 99.99% of the elements are zero. Thisfact makes the code very inefficent.   

2.)Compile and train the model from the tensorflow exercise.  Plot the training and validation loss as well as accuracy.  Post your plots and describe them.

The First thing to note about this model is how the model is overfitting. One can see this in the list of the printed out epochs. With this approach, the model reaches a validation accuracy of around 88%, and our traning accuracy(accuracy: 0.9607) is significantly heigher. The larger training accuracy  indicates over fitting, and The validation decrease and the steep increase also indicates over fitting.   

![July_28_20_Q2](https://github.com/Acejv21/Ace_Code/blob/master/Jul_28_20_Q2..png?raw=true)
![July_28_20_Q2_2](https://github.com/Acejv21/Ace_Code/blob/master/Jul_28_20_Q2_2.png?raw=true)



3.Stretch Goal:  Follow the link to the Embedding Projector provided at the end of the exercise.  Produce the visualization of your embeddings.  Interpret your visualization.  What is it describing?  Is there relevance with regard to words that are proximate to each other?

D.

1.) Text Classification with an RNN1.Again compile and train the model from the tensorflow exercise.  Plot the training and validation loss as well as accuracy.  Stack two or more LSTM layers in your model.  Post your plots and describe them.


![July_28_20_Q3_1](https://github.com/Acejv21/Ace_Code/blob/master/July_28_Q3_1.png?raw=true)
![July_28_20_Q3_2](https://github.com/Acejv21/Ace_Code/blob/master/July_28_Q3_2.png?raw=true)
