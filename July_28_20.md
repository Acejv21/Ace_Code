1.) Why is using one-hot encoding an inefficient towards vectorizing a corpus of words?  How are word embeddings different? (see this video https://www.youtube.com/watch?v=EEk6OiOOT2c )

A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine we have 10,000 words in the vocabulary. To one-hot encode each word, we would create a vector where 99.99% of the elements are zero. Thisfact makes the code very inefficent.   

2.)Compile and train the model from the tensorflow exercise.  Plot the training and validation loss as well as accuracy.  Post your plots and describe them.

![July_28_20_Q2](https://github.com/Acejv21/Ace_Code/blob/master/Jul_28_20_Q2..png?raw=true)
![July_28_20_Q2_2](https://github.com/Acejv21/Ace_Code/blob/master/Jul_28_20_Q2_2.png?raw=true)

3.Stretch Goal:  Follow the link to the Embedding Projector provided at the end of the exercise.  Produce the visualization of your embeddings.  Interpret your visualization.  What is it describing?  Is there relevance with regard to words that are proximate to each other?

D.

1.) Text Classification with an RNN1.Again compile and train the model from the tensorflow exercise.  Plot the training and validation loss as well as accuracy.  Stack two or more LSTM layers in your model.  Post your plots and describe them.
